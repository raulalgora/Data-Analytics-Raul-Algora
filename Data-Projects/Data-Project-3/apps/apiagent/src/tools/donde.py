import os
import google.generativeai as genai
import json
import re
from typing import TypedDict, Annotated, List, Union, Optional
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
import uuid
import requests

GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")

system_prompt_explorador = """
Rol y Objetivo Primario

Eres el "Explorador de Destinos", un asistente de viajes experto, amigable y muy perspicaz. Tu √öNICO objetivo principal es ayudar a los usuarios a descubrir y elegir un DESTINO DE VIAJE IDEAL Y ESPEC√çFICO (una CIUDAD concreta) basado en sus preferencias, intereses y necesidades. Una vez que el usuario haya confirmado esta CIUDAD espec√≠fica, tu tarea habr√° concluido.

Capacidades Adicionales

B√∫squeda de Informaci√≥n: Si necesitas m√°s detalles sobre un destino potencial que el usuario est√° considerando, para comparar opciones, o para enriquecer tu descripci√≥n de un lugar, **no dudes en solicitar una b√∫squeda**. Para hacerlo, en tu respuesta, incluye la frase [BUSCAR_INFO: "t√©rmino de b√∫squeda espec√≠fico aqu√≠"]. Por ejemplo: [BUSCAR_INFO: "mejores playas cerca de Lisboa en julio"] o [BUSCAR_INFO: "actividades culturales en Kioto para familias"]. Yo realizar√© la b√∫squeda y te dar√© los resultados para que contin√∫es la conversaci√≥n. **Utiliza esta herramienta libremente siempre que creas que puede aportar valor al usuario para definir su destino o conocerlo mejor.**

**MUY IMPORTANTE - GESTI√ìN DE RESULTADOS DE B√öSQUEDA:**
1. Cuando YO (el sistema) te proporcione resultados de una b√∫squeda que solicitaste con [BUSCAR_INFO: ...], tu SIGUIENTE RESPUESTA DEBE enfocarse en UTILIZAR Y PRESENTAR esos resultados al usuario para ayudarle a decidir sobre un destino.
2. NO DEBES REPETIR la misma solicitud [BUSCAR_INFO: ...] en la respuesta inmediata despu√©s de recibir los resultados para esa misma consulta. Asume que ya tienes la informaci√≥n.
3. Si los resultados que te proporcion√© no son suficientes o no son lo que esperabas, explica al usuario qu√© informaci√≥n adicional necesitas para ayudarle a elegir un destino y, si es necesario, formula una solicitud [BUSCAR_INFO: ...] *diferente* y m√°s espec√≠fica en tu siguiente turno, o pide al usuario que aclare. NUNCA repitas la misma b√∫squeda inmediatamente.

Tono y Estilo de Comunicaci√≥n

Cercano y conversacional, como un amigo entusiasta que adora viajar.
Evita lenguaje corporativo o respuestas gen√©ricas.
Utiliza emoji ocasionalmente para dar vida a la conversaci√≥n üå¥‚úàÔ∏èüèûÔ∏è.
Personalidad c√°lida pero profesional, transmitiendo pasi√≥n por los viajes.
Ocasionalmente comparte peque√±as an√©cdotas o datos curiosos sobre destinos.

Proceso de Interacci√≥n: Definici√≥n del Destino

Tu √öNICA Y M√ÅS IMPORTANTE TAREA es ayudar al usuario a elegir una CIUDAD espec√≠fica. Haz preguntas sobre sus intereses (ej. playa, monta√±a, cultura, aventura, relax), tipo de viaje deseado, presupuesto general inicial (si lo mencionan espont√°neamente, si no, no es prioritario), y cualquier otra preferencia que ayude a concretar una CIUDAD.
**Intenta recordar los intereses principales que el usuario asocia con su elecci√≥n final de CIUDAD.**

**Usa tu capacidad de [BUSCAR_INFO: ...] proactivamente para obtener m√°s datos sobre CIUDADES que puedan encajar o que el usuario mencione, incluso si no lo pide directamente. El objetivo es ofrecer la mayor cantidad de informaci√≥n relevante posible para ayudarle a decidir.**

Si el usuario da respuestas vagas sobre el destino, sigue preguntando y ofreciendo sugerencias (apoy√°ndote en b√∫squedas si es √∫til) hasta que se decida por una CIUDAD espec√≠fica (e.g., "Par√≠s", "Kioto", "Roma", "Barcelona").

NO intentes recopilar NINGUNA OTRA INFORMACI√ìN (como pa√≠s de origen, fechas, n√∫mero de viajeros, etc., a menos que sea estrictamente para diferenciar entre dos ciudades con el mismo nombre, y solo si es absolutamente necesario para definir la CIUDAD). Tu labor termina una vez que la CIUDAD est√° clara y confirmada.

El objetivo es que el campo "Destino Elegido" del resumen final sea el nombre de una CIUDAD real y espec√≠fica, y no frases como "a√∫n por decidir", "un lugar c√°lido" o un pa√≠s/regi√≥n.

Finalizaci√≥n de la Conversaci√≥n

Una vez que el usuario haya elegido una CIUDAD espec√≠fica y la haya CONFIRMADO CLARAMENTE (por ejemplo, diciendo "S√≠, quiero ir a Par√≠s" o "Perfecto, elijamos Tokio"), y SOLO ENTONCES:
1. En tu respuesta, felic√≠talo por su elecci√≥n.
2. Confirma CLARAMENTE la CIUDAD elegida.
3. Si el usuario mencion√≥ intereses espec√≠ficos para esa CIUDAD durante la conversaci√≥n, intenta incluirlos brevemente en tu confirmaci√≥n (ej. "¬°Excelente elecci√≥n ir a Kioto para disfrutar de sus templos y jardines!").
4. Finaliza la conversaci√≥n INMEDIATAMENTE despu√©s de esta confirmaci√≥n. No hagas m√°s preguntas.

Al finalizar, despu√©s de que el usuario confirme la CIUDAD y t√∫ hayas dado tu mensaje de confirmaci√≥n final, tu respuesta DEBE incluir un resumen MUY BREVE con EXACTAMENTE este formato:

Destino Elegido: [CIUDAD ESPEC√çFICA ELEGIDA]
Intereses Clave en Destino: [Intereses mencionados por el usuario para esa CIUDAD, si los hubo, o "No especificados"]

Es CRUCIAL que el campo "Destino Elegido" NUNCA est√© vac√≠o o sea una frase gen√©rica. Debe ser una CIUDAD espec√≠fica. Si el usuario elige un pa√≠s o regi√≥n, gu√≠alo suavemente para que elija una ciudad dentro de ese pa√≠s/regi√≥n.

Restricciones Clave

Prioridad Absoluta al Destino (CIUDAD): La elecci√≥n de una CIUDAD espec√≠fica es tu √öNICO objetivo. No te desv√≠es.
Nunca pidas informaci√≥n adicional: No cuestiones al usuario sobre otros aspectos del viaje (fechas, presupuesto, etc.) a menos que sea absolutamente necesario para definir la CIUDAD.
No pidas m√°s informaci√≥n: Una vez que el usuario confirme la CIUDAD, no solicites m√°s detalles. Tu trabajo ha terminado.
No pidas informaci√≥n para el itinerario: No cuestiones sobre itinerarios, actividades o preferencias de viaje. Tu enfoque es la CIUDAD.
NO generar resumen sin CIUDAD clara: Bajo NINGUNA circunstancia generes el resumen final si el campo "Destino Elegido" no es una CIUDAD concreta y confirmada.
Confirmaci√≥n Expl√≠cita: Espera a que el usuario confirme expl√≠citamente su elecci√≥n de CIUDAD antes de finalizar.
Finalizaci√≥n Inmediata: Una vez que confirmes la elecci√≥n del usuario, termina la conversaci√≥n. No sigas preguntando.
NO recopilar datos adicionales: Una vez elegida la CIUDAD, no pidas m√°s informaci√≥n. Tu trabajo ha terminado.
Formato del Resumen Final: El resumen final DEBE seguir el formato especificado con exactitud.
"""

# --- Helper Functions del Explorador ---
def extract_final_destination_info(text: str) -> dict:
    data = {"Destino Elegido": "", "intereses": ""} 
    dest_match = re.search(r"Destino Elegido:\s*(.*?)(?=\nIntereses Clave en Destino:|\n\s*$|$)", text, re.IGNORECASE | re.DOTALL)
    if dest_match: data["Destino Elegido"] = dest_match.group(1).strip()
    intereses_match = re.search(r"Intereses Clave en Destino:\s*(.*?)(?=\n\s*$|$)", text, re.IGNORECASE | re.DOTALL)
    if intereses_match: data["intereses"] = intereses_match.group(1).strip()
    return data

def save_to_json(data: dict, filename="travel_data_discovery_langgraph.json"):
    try:
        with open(filename, "w", encoding="utf-8") as f: json.dump(data, f, indent=4, ensure_ascii=False)
        print(f"\n[INFO] Explorador: Datos guardados en '{filename}'")
        return True
    except Exception as e:
        print(f"\n[ERROR] Explorador: No se pudo guardar en JSON: {e}")
        return False

def tavily_search(query: str) -> str:
    if not TAVILY_API_KEY: return "Error: Tavily API key no configurada."
    try:
        print(f"[EXPLORADOR_TOOL_CALL] Tavily search: \"{query}\"")
        response = requests.post("https://api.tavily.com/search", json={ "api_key": TAVILY_API_KEY, "query": query, "search_depth": "basic", "include_answer": True, "max_results": 5 })
        response.raise_for_status()
        results = response.json()
        output_parts = []
        if results.get("answer"): output_parts.append(f"Respuesta directa de Tavily: {results['answer']}")
        if results.get("results"):
            sources_info = []
            for r in results["results"]: sources_info.append(f"- T√≠tulo: {r.get('title', 'N/A')}\n  URL: {r.get('url', 'N/A')}\n  Contenido: {r.get('content', 'No hay contenido adicional aqu√≠.')[:300]}...")
            if sources_info: output_parts.append("Fuentes y fragmentos relevantes:\n" + "\n".join(sources_info))
        if not output_parts: return "No se encontraron resultados directos o detallados en Tavily."
        return "\n\n".join(output_parts)
    except Exception as e: return f"Error durante la b√∫squeda con Tavily: {e}"

# --- LangGraph State Definition del Explorador ---
class TravelAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], lambda x, y: x + y]
    user_input: str
    api_configured: bool
    summary_detected: bool 
    extracted_data: dict 
    final_answer_generated: bool 
    search_query: Optional[str]
    search_results: Optional[str]
    has_searched_this_invoke: bool
    last_executed_search_query: Optional[str]

# --- Variables Globales para el Grafo y Modelo del Explorador ---
_explorer_llm_model = None
_explorer_app = None


def explorador_node(state: TravelAgentState) -> dict:
    global _explorer_llm_model
    if not _explorer_llm_model:
        return {"messages": [AIMessage(content="Error: LLM del Explorador no inicializado.")], "summary_detected": False, "search_query": None, "search_results": None, "has_searched_this_invoke": state.get("has_searched_this_invoke", False), "last_executed_search_query": state.get("last_executed_search_query")}

    current_messages = list(state["messages"])
    last_executed_query = state.get("last_executed_search_query")

    if state.get("search_results"):
        print(f"[EXPLORADOR_DEBUG] A√±adiendo resultados de b√∫squeda (para query: '{last_executed_query}') al contexto del LLM.")
        current_messages.append(SystemMessage(content=f"[Resultados de la b√∫squeda para \"{last_executed_query}\"]:\n{state['search_results']}"))
    
    history_for_gemini = []
    for msg in current_messages:
        if isinstance(msg, SystemMessage) and msg.content != system_prompt_explorador:
             history_for_gemini.append({'role': 'user', 'parts': [f"[CONTEXTO DEL SISTEMA/HERRAMIENTA]:\n{msg.content}"]})
        elif isinstance(msg, HumanMessage):
            history_for_gemini.append({'role': 'user', 'parts': [msg.content]})
        elif isinstance(msg, AIMessage):
            history_for_gemini.append({'role': 'model', 'parts': [msg.content]})

    ai_response_text = ""
    try:
        if not history_for_gemini: 
             last_user_message_content = [state.get("user_input", "Hola")]
             if len(history_for_gemini) <= 1:
                  chat_session = _explorer_llm_model.start_chat(history=[])
                  last_user_message_content = history_for_gemini[0]['parts'] if history_for_gemini else [state.get("user_input", "Hola")]
             else:
                  chat_session = _explorer_llm_model.start_chat(history=history_for_gemini[:-1])
                  last_user_message_content = history_for_gemini[-1]['parts']

        else:
            # Si el √∫ltimo mensaje es del modelo, pero necesitamos enviar uno del usuario (el actual state["user_input"])
            # esto es manejado por c√≥mo se construye `state_for_invoke` en la herramienta.
            # `state_for_invoke["messages"]` ya tendr√° el √∫ltimo HumanMessage.
             chat_session = _explorer_llm_model.start_chat(history=history_for_gemini[:-1] if len(history_for_gemini) > 1 else [])
             last_user_message_content = history_for_gemini[-1]['parts']


        print(f"[EXPLORADOR_DEBUG] Enviando a Gemini (√∫ltimo mensaje/contexto): {str(last_user_message_content)[:300]}...")
        # Aqu√≠, send_message espera el contenido actual del usuario.
        # La forma en que se construye `state_for_invoke` en la herramienta asegura que `messages`
        # termina con el HumanMessage actual, as√≠ que `history_for_gemini[-1]` es el correcto.
        response = chat_session.send_message(last_user_message_content)
        ai_response_text = response.text
    except Exception as e:
        print(f"Explorador: Error generando response de Gemini: {e}")
        ai_response_text = "Uff, parece que mis mapas mentales se mezclaron un poco. ¬øPodr√≠as repetirme eso?"

    search_request_match = re.search(r'\[BUSCAR_INFO:\s*\"(.*?)\"\]', ai_response_text)
    search_query_for_tool = None
    if search_request_match:
        potential_query = search_request_match.group(1).strip()
        if state.get("search_results") and last_executed_query and potential_query.lower() == last_executed_query.lower():
            print(f"[EXPLORADOR_INFO] LLM intent√≥ b√∫squeda redundante para '{potential_query}'. Se eliminar√°.")
            ai_response_text = ai_response_text.replace(search_request_match.group(0), "").strip()
        else:
            search_query_for_tool = potential_query
            print(f"[EXPLORADOR_INFO] LLM solicit√≥ b√∫squeda: '{search_query_for_tool}'")
    
    summary_detected_flag = False
    if "Destino Elegido:" in ai_response_text and "Intereses Clave en Destino:" in ai_response_text:
        if not search_query_for_tool:
            summary_detected_flag = True
            print("[EXPLORADOR_INFO] Detectado formato de resumen final.")
        else:
            print("[EXPLORADOR_WARNING] Formato de resumen detectado, pero tambi√©n b√∫squeda. Ignorando resumen.")
            
    if summary_detected_flag:
        search_query_for_tool = None

    return {
        "messages": [AIMessage(content=ai_response_text)],
        "summary_detected": summary_detected_flag,
        "search_query": search_query_for_tool,
        "search_results": None,
        "has_searched_this_invoke": state.get("has_searched_this_invoke", False),
        "last_executed_search_query": state.get("last_executed_search_query")
    }

# --- WEB SEARCH NODE ---
def web_search_node(state: TravelAgentState) -> dict:
    query_to_execute = state.get("search_query")
    if not query_to_execute:
        return {"search_results": "No se proporcion√≥ una consulta de b√∫squeda.", "search_query": None, "has_searched_this_invoke": state.get("has_searched_this_invoke", False), "last_executed_search_query": state.get("last_executed_search_query")}
    print(f"[EXPLORADOR_INFO] Ejecutando b√∫squeda web para: '{query_to_execute}'")
    tavily_results_text = tavily_search(query_to_execute)
    return {"search_results": tavily_results_text, "search_query": None, "has_searched_this_invoke": True, "last_executed_search_query": query_to_execute }

# --- EXTRACT DATA NODE ---
def extract_data_node(state: TravelAgentState) -> dict:
    current_has_searched_flag = state.get("has_searched_this_invoke", False)
    current_last_executed_query = state.get("last_executed_search_query")
    current_extracted_data = state.get("extracted_data", {})
    final_answer_generated_flag = False

    if state["summary_detected"]:
        last_ai_message_content = ""
        # El estado 'messages' en este punto solo deber√≠a tener la √∫ltima AIMessage del explorador_node
        if state["messages"] and isinstance(state["messages"][-1], AIMessage):
            last_ai_message_content = state["messages"][-1].content
        
        if not last_ai_message_content: print("[EXPLORADOR_ERROR] No se encontr√≥ mensaje de IA para extraer datos.")
        else:
            print("\n[EXPLORADOR_INFO] Resumen final detectado. Extrayendo...")
            extracted = extract_final_destination_info(last_ai_message_content)
            current_extracted_data = extracted
            destination_value = extracted.get("Destino Elegido", "").strip()
            invalid_destination_placeholders = ["", "a√∫n por decidir", "un lugar c√°lido", "[destino espec√≠fico elegido]", "destino espec√≠fico elegido", "no especificados", "[ciudad espec√≠fica elegida]"] 
            if destination_value and destination_value.lower() not in invalid_destination_placeholders:
                print("\n[EXPLORADOR_RESUMEN_FINAL] Extra√≠do:")
                print(f"  - Destino Elegido: {extracted.get('Destino Elegido', '[vac√≠o]')}")
                print(f"  - intereses: {extracted.get('intereses', '[vac√≠o]')}")
                final_answer_generated_flag = True
            else:
                print(f"[EXPLORADOR_WARNING] 'Destino Elegido' ('{destination_value}') inv√°lido en resumen.")
                state["summary_detected"] = False
    return {"extracted_data": current_extracted_data, "final_answer_generated": final_answer_generated_flag, "summary_detected": state["summary_detected"], "has_searched_this_invoke": current_has_searched_flag,  "last_executed_search_query": current_last_executed_query}

def save_data_node(state: TravelAgentState) -> dict:
    if state["final_answer_generated"] and state["extracted_data"].get("Destino Elegido"):
        if save_to_json(state["extracted_data"]): print("[EXPLORADOR_INFO] Guardado JSON de resumen final exitoso.")
        else: print("[EXPLORADOR_ERROR] Fallo al guardar JSON el resumen final.")
    else: print("[EXPLORADOR_INFO] No se guardaron datos: no es respuesta final o Destino Elegido falta.")
    return {"has_searched_this_invoke": state.get("has_searched_this_invoke", False), "last_executed_search_query": state.get("last_executed_search_query")}

# --- CONDITIONAL EDGES ---
def route_after_explorador(state: TravelAgentState) -> str:
    llm_requested_search = state.get("search_query")
    already_searched_this_invoke = state.get("has_searched_this_invoke", False)
    if state.get("summary_detected"):
        print("[EXPLORADOR_ROUTE] Explorador -> Extract Data (resumen final)")
        return "extract_data"
    if llm_requested_search and not already_searched_this_invoke:
        print("[EXPLORADOR_ROUTE] Explorador -> Perform Search")
        return "perform_search"
    if llm_requested_search and already_searched_this_invoke:
        print("[EXPLORADOR_ROUTE] Explorador -> END (ya se busc√≥ en este invoke)")
    else:
        print("[EXPLORADOR_ROUTE] Explorador -> END (sin b√∫squeda o resumen)")
    return END

def route_after_extraction(state: TravelAgentState) -> str:
    if state.get("final_answer_generated") and state.get("extracted_data", {}).get("Destino Elegido"):
        print("[EXPLORADOR_ROUTE] Extract Data -> Save Data")
        return "save_data"
    print("[EXPLORADOR_ROUTE] Extract Data -> END (extracci√≥n fallida o no final)")
    return END


# --- Funci√≥n de Inicializaci√≥n para el Explorador ---
def initialize_destination_explorer():
    global _explorer_llm_model, _explorer_app
    if _explorer_app: # Prevenir reinicializaci√≥n
        print("Explorador de Destinos ya inicializado.")
        return

    print("Inicializando Explorador de Destinos...")
    try:
        if not GOOGLE_API_KEY:
            print("Explorador de Destinos: GOOGLE_API_KEY no encontrado.")
            return
        genai.configure(api_key=GOOGLE_API_KEY)
        _explorer_llm_model = genai.GenerativeModel(
            model_name="gemini-2.0-flash",
            safety_settings={'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'},
            system_instruction=system_prompt_explorador
        )
        print("Explorador de Destinos: Modelo Gemini configurado.")
    except Exception as e:
        print(f"Explorador de Destinos: Error configurando Gemini - {e}")
        _explorer_llm_model = None
        return # No continuar si el LLM falla

    # Construir el grafo del explorador
    workflow = StateGraph(TravelAgentState)
    workflow.add_node("explorador", explorador_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("extract_data", extract_data_node)
    workflow.add_node("save_data", save_data_node)
    
    workflow.set_entry_point("explorador")
    
    workflow.add_conditional_edges(
        "explorador", 
        route_after_explorador, # Tu funci√≥n de enrutamiento
        {
            "perform_search": "web_search", 
            "extract_data": "extract_data", 
            END: END
        }
    )
    workflow.add_edge("web_search", "explorador") # El nodo de b√∫squeda vuelve al explorador
    workflow.add_conditional_edges(
        "extract_data", 
        route_after_extraction, # Tu funci√≥n de enrutamiento
        {
            "save_data": "save_data", 
            END: END
        }
    )
    workflow.add_edge("save_data", END)

    _explorer_app = workflow.compile()
    print("Explorador de Destinos: Grafo LangGraph compilado y listo.")


# --- Herramienta para el Agente Principal ---
@tool
def destination_explorer_tool(user_input: str, current_explorer_state_messages: Optional[List[dict]] = None) -> dict:
    """
    Ayuda al usuario a elegir un destino de viaje (una CIUDAD espec√≠fica).
    Utiliza esta herramienta cuando el usuario quiera explorar opciones de destino
    pero a√∫n no tenga una ciudad en mente, o cuando necesite continuar una conversaci√≥n
    previa con el explorador de destinos.
    Debes pasar la √∫ltima entrada del usuario ('user_input').
    Si est√°s continuando una conversaci√≥n, pasa el historial de mensajes del explorador
    como 'current_explorer_state_messages' (una lista de diccionarios {'role': ..., 'parts': ...}).
    La herramienta devolver√° la respuesta del "Explorador de Destinos", su nuevo historial de mensajes
    para la siguiente llamada, y si la tarea de exploraci√≥n ha finalizado.
    """
    if _explorer_app is None:
        initialize_destination_explorer()
        if _explorer_app is None:
            return {
                "explorer_response": "Error cr√≠tico: El Explorador de Destinos no pudo inicializarse.",
                "updated_explorer_messages_history": current_explorer_state_messages or [],
                "is_finished": True,
                "final_data": None
            }

    # Construir el estado inicial o continuado para el grafo del explorador
    messages_for_explorer_graph = []
    last_search_results_for_graph = None
    last_executed_query_for_graph = None
    
    if current_explorer_state_messages:
        # Reconstruir los mensajes BaseMessage y otros campos del estado a partir del historial
        for msg_dict in current_explorer_state_messages:
            if msg_dict.get("role") == "user_internal_context":
                # Asumimos que este contexto contiene search_results y last_executed_query
                last_search_results_for_graph = msg_dict.get("search_results_content")
                last_executed_query_for_graph = msg_dict.get("last_executed_query_content")
                # No lo a√±adimos directamente a messages_for_explorer_graph como mensaje visible al LLM
            elif msg_dict.get("role") == "user":
                messages_for_explorer_graph.append(HumanMessage(content=msg_dict["parts"][0]))
            elif msg_dict.get("role") == "model":
                 messages_for_explorer_graph.append(AIMessage(content=msg_dict["parts"][0]))
            # Los SystemMessage del prompt del explorador no se pasan aqu√≠, se configuran en el modelo.

    # A√±adir el input actual del usuario
    messages_for_explorer_graph.append(HumanMessage(content=user_input))

    state_for_invoke: TravelAgentState = {
        "messages": messages_for_explorer_graph,
        "user_input": user_input,
        "api_configured": True,
        "summary_detected": False,
        "extracted_data": {},
        "final_answer_generated": False,
        "search_query": None,
        "search_results": last_search_results_for_graph,
        "has_searched_this_invoke": False,
        "last_executed_search_query": last_executed_query_for_graph
    }
    
    if not current_explorer_state_messages:
        initial_ai_greeting = "¬°Hola! Soy tu Explorador de Destinos üß≠. ¬øTienes ganas de viajar pero no sabes a qu√© CIUDAD? ¬°Est√°s en el lugar correcto! Cu√©ntame un poco qu√© te apetece para tu escapada urbana: ¬øcultura üèõÔ∏è, gastronom√≠a üçú, vida nocturna üéâ, relax en parques üå≥...? ¬°Vamos a descubrir tu ciudad ideal juntos!"
        # Prepend el saludo del explorador para que el LLM lo vea en el primer turno REAL de usuario.
        # El estado messages ya tiene el HumanMessage, as√≠ que lo insertamos antes.
        state_for_invoke["messages"].insert(0, AIMessage(content=initial_ai_greeting))

    try:
        graph_output_state = _explorer_app.invoke(state_for_invoke)

        # La respuesta para el usuario es el √∫ltimo mensaje de IA del explorador
        explorer_response_to_user = ""
        if graph_output_state["messages"] and isinstance(graph_output_state["messages"][-1], AIMessage):
            explorer_response_to_user = graph_output_state["messages"][-1].content
        
        is_finished = graph_output_state.get("final_answer_generated", False) and \
                      graph_output_state.get("extracted_data", {}).get("Destino Elegido")
        
        final_data = None
        if is_finished:
            final_data = graph_output_state.get("extracted_data")
            print(f"[EXPLORER_TOOL] Tarea finalizada. Datos: {final_data}")

        # Preparar el historial de mensajes para la siguiente posible llamada a la herramienta
        # Esto debe capturar el estado de la conversaci√≥n DENTRO del explorador.
        # Usamos el formato de historial de Gemini para simplicidad.

        updated_messages_history_for_next_call = []
        raw_messages_from_graph = graph_output_state.get("messages", [])

        # Reconstruimos el historial que el explorador ha acumulado en su estado 'messages' y lo pasamos para la siguiente llamada.
        # Necesitamos reconstruir desde el graph_output_state['messages'] que es la
        # lista COMPLETA de mensajes que el explorador us√≥/gener√≥.

        for msg in raw_messages_from_graph:
            if isinstance(msg, HumanMessage):
                updated_messages_history_for_next_call.append({'role': 'user', 'parts': [msg.content]})
            elif isinstance(msg, AIMessage):
                updated_messages_history_for_next_call.append({'role': 'model', 'parts': [msg.content]})
        
        # Si hubo una b√∫squeda, necesitamos preservar sus resultados para el siguiente turno del explorador
        # dentro del historial que pasamos, de una manera que el explorador_node pueda usarlo.
        
        if graph_output_state.get("search_results") and graph_output_state.get("last_executed_search_query"):
            updated_messages_history_for_next_call.append({
                "role": "user_internal_context", # Rol especial para que no se muestre al LLM como mensaje directo
                "parts": ["Contexto de b√∫squeda anterior"],
                "search_results_content": graph_output_state.get("search_results"),
                "last_executed_search_query_content": graph_output_state.get("last_executed_search_query")
            })


        return {
            "explorer_response": explorer_response_to_user,
            "updated_explorer_messages_history": updated_messages_history_for_next_call,
            "is_finished": is_finished,
            "final_data": final_data
        }

    except Exception as e:
        import traceback
        print(f"[ERROR CR√çTICO DENTRO DE destination_explorer_tool] Error: {e}")
        traceback.print_exc()
        return {
            "explorer_response": "¬°Vaya! El Explorador de Destinos tuvo un problema interno. Quiz√°s necesitemos reiniciar la exploraci√≥n.",
            "updated_explorer_messages_history": [],
            "is_finished": True, 
            "final_data": None
        }
